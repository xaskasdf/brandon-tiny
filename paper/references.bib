% Brandon-Tiny 10M â€” References

@inproceedings{mobilellm2024,
  title={MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases},
  author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and Lai, Liangzhen and Chandra, Vikas},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{smollm2-2025,
  title={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model},
  author={Allal, Loubna Ben and Lozhkov, Anton and Bakouch, Elie and Mart{\'i}n Bl{\'a}zquez, Gabriel and Penedo, Guilherme and Tunstall, Lewis and Marafioti, Andr{\'e}s and others},
  journal={arXiv preprint arXiv:2502.02737},
  year={2025}
}

@article{supertiny2024,
  title={Super Tiny Language Models},
  author={Hillier, Dylan and Guertler, Leon and Tan, Cheston and Agrawal, Palaash and Chen, Ruirui and Cheng, Bobby},
  journal={arXiv preprint arXiv:2405.14159},
  year={2024}
}

@article{denseformer2024,
  title={DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging},
  author={Pagliardini, Matteo and Mohtashami, Amirkeivan and Fleuret, Fran{\c{c}}ois and Jaggi, Martin},
  journal={arXiv preprint arXiv:2402.02622},
  year={2024}
}

@article{valueresidual2024,
  title={Value Residual Learning For Alleviating Attention Concentration In Transformers},
  author={Wang, Zhengqi and Li, Yuxia and Liu, Rongzhi},
  journal={arXiv preprint arXiv:2410.17897},
  year={2024},
  note={Accepted at ACL 2025}
}

@inproceedings{registers2024,
  title={Vision Transformers Need Registers},
  author={Darcet, Timoth{\'e}e and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@article{minicpm2024,
  title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}

@inproceedings{miniplm2025,
  title={MiniPLM: Knowledge Distillation for Pre-Training Language Models},
  author={Gu, Yuxian and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025},
  note={arXiv:2410.17215}
}

@inproceedings{unlikelihood2020,
  title={Neural Text Generation with Unlikelihood Training},
  author={Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}
