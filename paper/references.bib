% Brandon-Tiny 10M â€” References

@inproceedings{mobilellm2024,
  title={MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases},
  author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and Lai, Liangzhen and Chandra, Vikas},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{smollm2-2025,
  title={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model},
  author={Allal, Loubna Ben and Lozhkov, Anton and Bakouch, Elie and Mart{\'i}n Bl{\'a}zquez, Gabriel and Penedo, Guilherme and Tunstall, Lewis and Marafioti, Andr{\'e}s and others},
  journal={arXiv preprint arXiv:2502.02737},
  year={2025}
}

@article{supertiny2024,
  title={Super Tiny Language Models},
  author={Hillier, Dylan and Guertler, Leon and Tan, Cheston and Agrawal, Palaash and Chen, Ruirui and Cheng, Bobby},
  journal={arXiv preprint arXiv:2405.14159},
  year={2024}
}

@article{denseformer2024,
  title={DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging},
  author={Pagliardini, Matteo and Mohtashami, Amirkeivan and Fleuret, Fran{\c{c}}ois and Jaggi, Martin},
  journal={arXiv preprint arXiv:2402.02622},
  year={2024}
}

@article{valueresidual2024,
  title={Value Residual Learning For Alleviating Attention Concentration In Transformers},
  author={Wang, Zhengqi and Li, Yuxia and Liu, Rongzhi},
  journal={arXiv preprint arXiv:2410.17897},
  year={2024},
  note={Accepted at ACL 2025}
}

@inproceedings{registers2024,
  title={Vision Transformers Need Registers},
  author={Darcet, Timoth{\'e}e and Oquab, Maxime and Mairal, Julien and Bojanowski, Piotr},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@article{minicpm2024,
  title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}

@inproceedings{miniplm2025,
  title={MiniPLM: Knowledge Distillation for Pre-Training Language Models},
  author={Gu, Yuxian and Zhou, Hao and Meng, Fandong and Zhou, Jie and Huang, Minlie},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025},
  note={arXiv:2410.17215}
}

@inproceedings{unlikelihood2020,
  title={Neural Text Generation with Unlikelihood Training},
  author={Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}

@article{llama2-2023,
  title={Llama 2: Open Foundation and Fine-Tuned Chat Models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{tinystories2023,
  title={TinyStories: How Small Can Language Models Be and Still Speak Coherent English?},
  author={Eldan, Ronen and Li, Yuanzhi},
  journal={arXiv preprint arXiv:2305.07759},
  year={2023}
}

@inproceedings{mtp2024,
  title={Better \& Faster Large Language Models via Multi-Token Prediction},
  author={Gloeckle, Fabian and Idrissi, Badr Youbi and Rozi{\`e}re, Baptiste and Lopez-Paz, David and Synnaeve, Gabriel},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{rope2024,
  title={RoFormer: Enhanced Transformer with Rotary Position Embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024}
}

@article{swiglu2020,
  title={GLU Variants Improve Transformer},
  author={Shazeer, Noam},
  journal={arXiv preprint arXiv:2002.05202},
  year={2020}
}

@inproceedings{gqa2023,
  title={GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints},
  author={Ainslie, Joshua and Lee-Thorp, James and de Jong, Michiel and Zemlyanskiy, Yury and Lebron, Federico and Sanghai, Sumit},
  booktitle={Empirical Methods in Natural Language Processing (EMNLP)},
  year={2023}
}
