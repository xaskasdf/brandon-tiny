% Brandon-Tiny 10M â€” References

@inproceedings{mobilellm2024,
  title={MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases},
  author={Liu, Zechun and Zhao, Changsheng and Iandola, Forrest and Lai, Chen and Tian, Yuandong and Fedorov, Igor and Xiong, Yunyang and Chang, Ernie and Shi, Yangyang and Krishnamoorthi, Raghuraman and Lai, Liangzhen and Chandra, Vikas},
  booktitle={International Conference on Machine Learning (ICML)},
  year={2024}
}

@article{smollm2-2025,
  title={SmolLM2: When Smol Goes Big -- Data-Centric Training of a Small Language Model},
  author={Allal, Loubna Ben and Lozhkov, Anton and Penedo, Guilherme and Wolf, Thomas and von Werra, Leandro},
  journal={arXiv preprint arXiv:2502.02737},
  year={2025}
}

@article{supertiny2024,
  title={Super Tiny Language Models},
  author={Galimzhanova, Mingxue and Parajuli, Neeraj and Goswami, Manas},
  journal={arXiv preprint arXiv:2405.14159},
  year={2024}
}

@article{denseformer2024,
  title={DenseFormer: Enhancing Information Flow in Transformers via Depth Weighted Averaging},
  author={Pagliardini, Matteo and Jaggi, Martin and Fleuret, Fran{\c{c}}ois and Karimireddy, Sai Praneeth},
  journal={arXiv preprint arXiv:2402.02622},
  year={2024}
}

@article{valueresidual2024,
  title={Value Residual Learning For Alleviating Attention Head Redundancy},
  author={Wang, Zhengqi and Li, Yuxia and Liu, Rongzhi},
  journal={arXiv preprint arXiv:2410.17276},
  year={2024}
}

@inproceedings{registers2024,
  title={Vision Transformers Need Registers},
  author={Darcet, Timoth{\'e}e and Oquab, Maxime and Mairal, Julien and Jegou, Herve},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2024}
}

@article{minicpm2024,
  title={MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies},
  author={Hu, Shengding and Tu, Yuge and Han, Xu and He, Chaoqun and Cui, Ganqu and Long, Xiang and Zheng, Zhi and Fang, Yewei and Huang, Yuxiang and Zhao, Weilin and others},
  journal={arXiv preprint arXiv:2404.06395},
  year={2024}
}

@inproceedings{miniplm2025,
  title={Knowledge Distillation for Small Language Models},
  author={{MiniPLM authors}},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2025}
}

@inproceedings{unlikelihood2020,
  title={Neural Text Generation with Unlikelihood Training},
  author={Welleck, Sean and Kulikov, Ilia and Roller, Stephen and Dinan, Emily and Cho, Kyunghyun and Weston, Jason},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2020}
}
