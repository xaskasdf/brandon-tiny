# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
*.egg-info/
.eggs/

# Data files (large - download with scripts/download_datasets.py)
data/tinystories/
data/smollm/
data/fineweb/
data/minipile/
data/wikitext/*.pkl
data/wikitext/*.bin
data/wikitext/train.txt
data/wikitext/validation.txt
data/wikitext/test.txt
data/wikipedia/
data/combined/
data/combined_10m/
data/combined_30m/
data/combined_30m_v2/
data/combined_110m/
data/synthetic_only/
data/synthetic_pretrain/
data/finetune_10m_optimal/
data/chat_curated/
*.parquet

# Checkpoints (large binary files - download from HuggingFace)
checkpoints/

# IDE
.vscode/
.idea/
*.swp
*.swo

# OS
.DS_Store
Thumbs.db

# Logs
*.log
wandb/

# Claude Code internal
.claude/
.playwright-mcp/

# LaTeX build artifacts
paper/*.aux
paper/*.bbl
paper/*.blg
paper/*.out
paper/*.fls
paper/*.fdb_latexmk
paper/*.synctex.gz

# Exports
exports/

# Keep tokenizer models and vocabs
!data/tokenizer_8k.model
!data/tokenizer_8k.vocab

# Large data files in chat/ (download from HuggingFace)
data/chat/train.jsonl
data/chat/val.jsonl
data/chat/alpaca.jsonl
data/chat/dolly.jsonl
data/chat/synthetic_10k.jsonl
data/chat/train_curated.jsonl
data/chat/val_curated.jsonl
data/test_synthetic.jsonl

# Keep sample data for reference
!data/chat/sample.jsonl

# Old tokenizer (1k vocab - superseded by 8k)
data/tokenizer_1k.model
data/tokenizer_1k.vocab

# Research docs (internal, not for public repo)
RESEARCH_DEEP_DIVE.md
RESEARCH_NEXT_STEPS.md
API_DOCS.md
